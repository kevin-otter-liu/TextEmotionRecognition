{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9721736",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a7f41",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64bea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 16:00:17.983281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:17.983303: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-02 16:00:19.235760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-02 16:00:19.236090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.236143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.236186: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.237675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.237719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.237759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-02 16:00:19.237765: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f6996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd27823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv(\"./data/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354d4ec",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Getting rid of the punctuations marks from dataset\n",
    "2. converting all content to lowercase\n",
    "3. converting unicode characters to ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09517d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.copy()\n",
    "\n",
    "# removing all punctuation marks\n",
    "exclude = set(string.punctuation)\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: regex.sub(\"\",sentence))\n",
    "\n",
    "# lowercasing 'content' column\n",
    "cleaned_df['content'] = cleaned_df['content'].str.lower()\n",
    "\n",
    "# removing unicode characters\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: sentence.encode(\"ascii\",\"ignore\").decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85871b38",
   "metadata": {},
   "source": [
    "1. Now we want to find the total number of unique words (vocab_size) to rationalise the size of a embedding vector\n",
    "1. We also want to find the maximum number of words that are in a sentence to justify the size of the input layer of the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a451c62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length of 'content' is 33\n",
      "total number of words of content is 522873\n",
      "total number of unique words/vocab_size of content is 53612\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = cleaned_df['content'].apply(lambda s: len(s.split())).max()\n",
    "no_of_words = cleaned_df['content'].apply(lambda content: len(content.split())).sum()\n",
    "\n",
    "\n",
    "# count vocab size\n",
    "cache=set()\n",
    "\n",
    "#counting padding and [unkownd] token\n",
    "vocab_size= 2\n",
    "for key,sentence in cleaned_df['content'].items():\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if(word not in cache):\n",
    "            vocab_size+=1\n",
    "            cache.add(word)\n",
    "            \n",
    "print(\"max sentence length of 'content' is {}\".format(max_sentence_len))\n",
    "print(\"total number of words of content is {}\".format(no_of_words))\n",
    "print(\"total number of unique words/vocab_size of content is {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b625945",
   "metadata": {},
   "source": [
    "Since our max sentence length is 33 we will make an embedding of shape (64,) and pad the difference of the sentence. <br>\n",
    "Since it is computationally heavy to have a vector of 63612 in size. <br>\n",
    "We will use a vector of only 64, meaning that in the skip_grams algorithm, the words will only take 64 other context words into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bad8ba",
   "metadata": {},
   "source": [
    "# using word2Vec to get continous vectors to use as embeddings instead of one-hot vectors\n",
    "> using continuous vectors instead of one-hot vectors is better as continuos vectors contain contextual meaning learned from the unsupervised learning performed in the word2vec training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12a678",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfcb67",
   "metadata": {},
   "source": [
    "Combine the steps to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f33c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size,num_ns,vocab_size,seed):\n",
    "    \n",
    "    # each training sentence is appended to these list\n",
    "    targets,contexts,labels = [],[],[]\n",
    "    \n",
    "    # sampling table for vocab_size tokens\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    \n",
    "    # iterate over all sentences in dataset\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "       \n",
    "\n",
    "        # generating positive skip-gram pairs for a sequence\n",
    "        positive_skip_grams,_ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size = vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size, # TODO: change window size\n",
    "            negative_samples=0\n",
    "        )\n",
    "        \n",
    "        # produce negative samples and create training samples (x_train,labels)\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    \n",
    "            # expand context word to frmo dim shape (1,0) to (1,1)\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word],dtype='int64'),1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class, # to tell the which sample is positive\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size, #TODO: may need to change to just the negative samples of the sentence itself instead of the entire vocab\n",
    "                seed=seed,\n",
    "                name='negative_sampling')\n",
    "    \n",
    "    \n",
    "            # building the context and label vectors for a target word\n",
    "            context = tf.concat([tf.squeeze(context_class,1),negative_sampling_candidates],0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "    \n",
    "    \n",
    "            # append each element from the training ex to global lists\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    \n",
    "    \n",
    "    return targets, contexts, labels \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb6ce",
   "metadata": {},
   "source": [
    "## Preparing training data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fed3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 16:00:30.215321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# size of one sentence is 33 but we just use 64\n",
    "sequence_length = 64\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f630be1",
   "metadata": {},
   "source": [
    "## Tokenise the words in content according to their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d95de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to smaller batch for better results\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d7fbd",
   "metadata": {},
   "source": [
    "### replace words with their respective tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc51038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset of all sentences\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(cleaned_df['content'])\n",
    "vectorize_layer.adapt(text_ds.batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b82457",
   "metadata": {},
   "source": [
    "### build a inverse vocab which maps indexes -> words which can be handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fdc089",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e9f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "# prefetch does fetching of data and training at the same time using multiple thread\n",
    "# improving performance\n",
    "text_vector_ds = text_ds.batch(batch_size).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8623f6",
   "metadata": {},
   "source": [
    "As you can see we have successfully vectorised our sentences/sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f064d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208726b",
   "metadata": {},
   "source": [
    "## Using unsupervised learning (word2Vec skip_gram) to predict context from targets. \n",
    ">While doing so, we are also training the weights on the embeddings. We can increase window size so that the embeddings learn more contextutal knowledge with respect to the words around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b60b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to determine window size, we see the median length of a sentence\n",
    "\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).median())\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda81652",
   "metadata": {},
   "source": [
    "We go with window size of 12 since that is the median length of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note higher window size is more computationally expensive\n",
    "# from documentation, it is said that for small datasets, negative samples of range 5to 20 yields the best results\n",
    "\n",
    "num_ns = 10\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=12,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978a0",
   "metadata": {},
   "source": [
    "Configuring training sets for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomising the dataset\n",
    "\n",
    "BATCH_SIZE = batch_size\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "              embedding_dim,\n",
    "              input_length=1,\n",
    "              name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "               embedding_dim,\n",
    "               input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d598c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding size for word2vec is chosen to be of shape (32,)\n",
    "# the idea is that since there are a maximum of 33 words in a sentece\n",
    "# the vector that a word takes will be in 32 dimension \n",
    "# vocab size +1 because of padding\n",
    "embedding_dim = 32\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "# call back to log training stats for TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df77dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = word2vec.fit(dataset, epochs=40, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1ea3",
   "metadata": {},
   "source": [
    "## Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5568f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('./data/vectors1.tsv', 'w')\n",
    "out_m = io.open('./data/metadata1.tsv', 'w')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7c0b",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52cb7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        result.append([float(value) for value in line.split()])\n",
    "        \n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7b6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = read_tsv('./data/vectors1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4182c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "            \n",
    "        result.append(line.strip())\n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ee5548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = read_meta('./data/metadata1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09a157dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53611"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150b508",
   "metadata": {},
   "source": [
    "## Preprocessing for GRU: one hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59f854e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels:\n",
      "0) empty\n",
      "1) sadness\n",
      "2) enthusiasm\n",
      "3) neutral\n",
      "4) worry\n",
      "5) surprise\n",
      "6) love\n",
      "7) fun\n",
      "8) hate\n",
      "9) happiness\n",
      "10) boredom\n",
      "11) relief\n",
      "12) anger\n"
     ]
    }
   ],
   "source": [
    "print('Unique labels:')\n",
    "for i, label in enumerate(cleaned_df['sentiment'].unique()):\n",
    "    print('{}) {}'.format(i,label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f0ffb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e0625",
   "metadata": {},
   "source": [
    "# Build a one hot vector each of size 13 for sentiments as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7034d0",
   "metadata": {},
   "source": [
    "### build a map for the labels to the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24674dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels -> indices\n",
    "sentiment_labels = cleaned_df['sentiment'].unique().copy()\n",
    "\n",
    "# index key -> class\n",
    "label_map = {}\n",
    "\n",
    "#class -> index key\n",
    "inverse_label_map = {}\n",
    "for i, label in enumerate(sentiment_labels):\n",
    "    label_map[i] = label \n",
    "    inverse_label_map[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "080fa0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all keys\n",
    "indices = []\n",
    "for key,value in label_map.items():\n",
    "    indices.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c2016",
   "metadata": {},
   "source": [
    "### Generates one-hot vector for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5467ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the word to the one hot vector\n",
    "depth = len(indices)\n",
    "one_hot_encoding = tf.one_hot(indices,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d31c9844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 13), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8cbcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 13])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06199e6",
   "metadata": {},
   "source": [
    "Map the one hot encodings to the labels in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "681f8578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'sadness', ..., 'love', 'happiness', 'love'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ab047b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:04<00:00, 8075.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# instantiate the preprocessed dataframe to feed into the model\n",
    "# cleaned_df['label'] = cleaned_df['sentiment'].apply(lambda n: one_hot_encoding[inverse_label_map[n]])\n",
    "# cleaned_df['label'] = cleaned_df['sentiment'].apply(lambda n: inverse_label_map[n])\n",
    "\n",
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    inner_list = []\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    for i in range(0,33):\n",
    "        inner_list.append(o_h)\n",
    "    labels.append(inner_list)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "#labels = np.asarray([(one_hot_encoding[inverse_label_map[sentiment]]) for sentiment in cleaned_df['sentiment'].values])\n",
    "# labels = np.asarray([tf.keras.utils.to_categorical([inverse_label_map[sentiment]],33) for sentiment in cleaned_df['sentiment'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08188aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 33, 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label shape is: \n",
    "# 40000 (no_of_sequences), 33(time_sequence which is also = max_length of sentence, classes_no)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518cb90",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70988b",
   "metadata": {},
   "source": [
    "## Convert Labels (sentiment) to tokenized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f8e637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53610 unique tokens\n",
      "labels: [[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "content_lines = list(cleaned_df['content'])\n",
    "tokeniser_obj = Tokenizer()\n",
    "tokeniser_obj.fit_on_texts(content_lines)\n",
    "sequences = tokeniser_obj.texts_to_sequences(content_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokeniser_obj.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "\n",
    "max_length = max_sentence_len\n",
    "content_pad = pad_sequences(sequences,maxlen=max_length)\n",
    "print(\"labels: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae6ab67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of content tensor: (40000, 33)\n",
      "shape of sentiment tensor: (40000, 33, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of content tensor: {}\".format(content_pad.shape))\n",
    "print(\"shape of sentiment tensor: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569be797",
   "metadata": {},
   "source": [
    "## Map embeddings from word2vec model for each word to the sequences (sentences) by createing a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38fde1",
   "metadata": {},
   "source": [
    "Map the embeddings to each word of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6af51fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an inverse index of the classes name -> index\n",
    "word_vector_map = {}\n",
    "for index,word in enumerate(classes):\n",
    "    word_vector_map[word] = vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cd76e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=32\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f328ca27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53611, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c65565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word,i in word_index.items():\n",
    "    if i> num_words:\n",
    "        continue\n",
    "    embedding_vector = word_vector_map.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        # if embedding vector is not found, \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156385b",
   "metadata": {},
   "source": [
    "# Model of GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbbe53",
   "metadata": {},
   "source": [
    "x (no_sequences,embeddings in a sequence=32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbc58f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to save the best weights for the model\n",
    "def saveWeightsCallback(path,monitor,mode,save_freq):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path,\n",
    "        monitor = monitor,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = mode,\n",
    "        save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f33f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def build_model(vocab_size,embedding_dim,max_length):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(num_words,\n",
    "                               EMBEDDING_DIM,\n",
    "                               embeddings_initializer=Constant(embedding_matrix),\n",
    "                               input_length = max_length,\n",
    "                              trainable=False))\n",
    "    model.add(layers.GRU(32,return_sequences=True, activation=\"tanh\"))\n",
    "    model.add(layers.Dense(13,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e54e8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 33, 32)            1715552   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 33, 32)            6336      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 33, 13)            429       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,722,317\n",
      "Trainable params: 6,765\n",
      "Non-trainable params: 1,715,552\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 33, 32)            1715552   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 33, 32)            6336      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 33, 13)            429       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,722,317\n",
      "Trainable params: 6,765\n",
      "Non-trainable params: 1,715,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GRU_model = build_model(vocab_size,EMBEDDING_DIM,max_length)\n",
    "\n",
    "GRU_model.compile(\n",
    "    optimizer= Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        'mean_squared_error'\n",
    "    ]\n",
    ")\n",
    "\n",
    "GRU_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c97fc97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29a4b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "indices = np.arange(content_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "content_pad = content_pad[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * content_pad.shape[0])\n",
    "\n",
    "X_train_pad = content_pad[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "X_test_pad = content_pad[-num_validation_samples:]\n",
    "y_test = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7346414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 - 3s - loss: 2.0969 - accuracy: 0.2539 - mean_squared_error: 0.0650 - val_loss: 2.1137 - val_accuracy: 0.2486 - val_mean_squared_error: 0.0652 - 3s/epoch - 14ms/step\n",
      "Epoch 2/100\n",
      "219/219 - 3s - loss: 2.0910 - accuracy: 0.2561 - mean_squared_error: 0.0649 - val_loss: 2.1117 - val_accuracy: 0.2501 - val_mean_squared_error: 0.0651 - 3s/epoch - 14ms/step\n",
      "Epoch 3/100\n",
      "219/219 - 3s - loss: 2.0861 - accuracy: 0.2581 - mean_squared_error: 0.0647 - val_loss: 2.1142 - val_accuracy: 0.2460 - val_mean_squared_error: 0.0652 - 3s/epoch - 14ms/step\n",
      "Epoch 4/100\n",
      "219/219 - 3s - loss: 2.0815 - accuracy: 0.2591 - mean_squared_error: 0.0647 - val_loss: 2.1121 - val_accuracy: 0.2486 - val_mean_squared_error: 0.0652 - 3s/epoch - 14ms/step\n",
      "Epoch 5/100\n",
      "219/219 - 3s - loss: 2.0783 - accuracy: 0.2604 - mean_squared_error: 0.0646 - val_loss: 2.1190 - val_accuracy: 0.2454 - val_mean_squared_error: 0.0653 - 3s/epoch - 14ms/step\n",
      "Epoch 6/100\n",
      "219/219 - 3s - loss: 2.0735 - accuracy: 0.2620 - mean_squared_error: 0.0645 - val_loss: 2.1156 - val_accuracy: 0.2487 - val_mean_squared_error: 0.0652 - 3s/epoch - 14ms/step\n",
      "Epoch 7/100\n",
      "219/219 - 3s - loss: 2.0699 - accuracy: 0.2629 - mean_squared_error: 0.0644 - val_loss: 2.1200 - val_accuracy: 0.2459 - val_mean_squared_error: 0.0653 - 3s/epoch - 14ms/step\n",
      "Epoch 8/100\n"
     ]
    }
   ],
   "source": [
    "history = GRU_model.fit(X_train_pad,\n",
    "                        y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs = 100,\n",
    "                        verbose=2,\n",
    "                        validation_data = (X_test_pad,y_test))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "                     saveWeightsCallback(\n",
    "                         path='./weights/GRU',\n",
    "                         monitor = 'val_loss',\n",
    "                         mode = 'min',\n",
    "                         save_freq='epoch',\n",
    "                     )],\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ebde0",
   "metadata": {},
   "source": [
    "## Brief description of method used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a4010",
   "metadata": {},
   "source": [
    "# LSTM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f8a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3813a7f",
   "metadata": {},
   "source": [
    "# Experiments and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798bfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetworks",
   "language": "python",
   "name": "neuralnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
