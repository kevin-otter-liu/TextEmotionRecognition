{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9721736",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a7f41",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64bea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 04:24:07.472942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:07.473034: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-03 04:24:09.960194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 04:24:09.960619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.960718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.960802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.978571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.978681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.978776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-03 04:24:09.978798: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f6996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd27823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv(\"./data/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354d4ec",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Getting rid of the punctuations marks from dataset\n",
    "2. converting all content to lowercase\n",
    "3. converting unicode characters to ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09517d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.copy()\n",
    "\n",
    "# lowercasing 'content' column\n",
    "cleaned_df['content'] = cleaned_df['content'].str.lower()\n",
    "\n",
    "# removing all words with #hastag and @name and urls\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub('@[A-Za-z0-9_]+','',sentence))\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub('#[A-Za-z0-9_]+','',sentence))\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub(r'http\\S+','',sentence))\n",
    "\n",
    "# removing all punctuation marks\n",
    "exclude = set(string.punctuation)\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: regex.sub(\"\",sentence))\n",
    "\n",
    "# removing unicode characters\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: sentence.encode(\"ascii\",\"ignore\").decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85871b38",
   "metadata": {},
   "source": [
    "1. Now we want to find the total number of unique words (vocab_size) to rationalise the size of a embedding vector\n",
    "1. We also want to find the maximum number of words that are in a sentence to justify the size of the input layer of the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a451c62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length of 'content' is 33\n",
      "total number of words of content is 522873\n",
      "total number of unique words/vocab_size of content is 53612\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = cleaned_df['content'].apply(lambda s: len(s.split())).max()\n",
    "no_of_words = cleaned_df['content'].apply(lambda content: len(content.split())).sum()\n",
    "\n",
    "\n",
    "# count vocab size\n",
    "cache=set()\n",
    "\n",
    "#counting padding and [unkownd] token\n",
    "vocab_size= 2\n",
    "for key,sentence in cleaned_df['content'].items():\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if(word not in cache):\n",
    "            vocab_size+=1\n",
    "            cache.add(word)\n",
    "            \n",
    "print(\"max sentence length of 'content' is {}\".format(max_sentence_len))\n",
    "print(\"total number of words of content is {}\".format(no_of_words))\n",
    "print(\"total number of unique words/vocab_size of content is {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b625945",
   "metadata": {},
   "source": [
    "Since our max sentence length is 33 we will make an embedding of shape (64,) and pad the difference of the sentence. <br>\n",
    "Since it is computationally heavy to have a vector of 63612 in size. <br>\n",
    "We will use a vector of only 64, meaning that in the skip_grams algorithm, the words will only take 64 other context words into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bad8ba",
   "metadata": {},
   "source": [
    "# using word2Vec to get continous vectors to use as embeddings instead of one-hot vectors\n",
    "> using continuous vectors instead of one-hot vectors is better as continuos vectors contain contextual meaning learned from the unsupervised learning performed in the word2vec training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12a678",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfcb67",
   "metadata": {},
   "source": [
    "Combine the steps to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f33c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size,num_ns,vocab_size,seed):\n",
    "    \n",
    "    # each training sentence is appended to these list\n",
    "    targets,contexts,labels = [],[],[]\n",
    "    \n",
    "    # sampling table for vocab_size tokens\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    \n",
    "    # iterate over all sentences in dataset\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "       \n",
    "\n",
    "        # generating positive skip-gram pairs for a sequence\n",
    "        positive_skip_grams,_ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size = vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size, # TODO: change window size\n",
    "            negative_samples=0\n",
    "        )\n",
    "        \n",
    "        # produce negative samples and create training samples (x_train,labels)\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    \n",
    "            # expand context word to frmo dim shape (1,0) to (1,1)\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word],dtype='int64'),1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class, # to tell the which sample is positive\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size, #TODO: may need to change to just the negative samples of the sentence itself instead of the entire vocab\n",
    "                seed=seed,\n",
    "                name='negative_sampling')\n",
    "    \n",
    "    \n",
    "            # building the context and label vectors for a target word\n",
    "            context = tf.concat([tf.squeeze(context_class,1),negative_sampling_candidates],0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "    \n",
    "    \n",
    "            # append each element from the training ex to global lists\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    \n",
    "    \n",
    "    return targets, contexts, labels \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb6ce",
   "metadata": {},
   "source": [
    "## Preparing training data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fed3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 04:24:14.645545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# size of one sentence is 33 but we just use 64\n",
    "sequence_length = 64\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f630be1",
   "metadata": {},
   "source": [
    "## Tokenise the words in content according to their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d95de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to smaller batch for better results\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d7fbd",
   "metadata": {},
   "source": [
    "### replace words with their respective tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc51038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset of all sentences\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(cleaned_df['content'])\n",
    "vectorize_layer.adapt(text_ds.batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b82457",
   "metadata": {},
   "source": [
    "### build a inverse vocab which maps indexes -> words which can be handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fdc089",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e9f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "# prefetch does fetching of data and training at the same time using multiple thread\n",
    "# improving performance\n",
    "text_vector_ds = text_ds.batch(batch_size).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8623f6",
   "metadata": {},
   "source": [
    "As you can see we have successfully vectorised our sentences/sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f064d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208726b",
   "metadata": {},
   "source": [
    "## Using unsupervised learning (word2Vec skip_gram) to predict context from targets. \n",
    ">While doing so, we are also training the weights on the embeddings. We can increase window size so that the embeddings learn more contextutal knowledge with respect to the words around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b60b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "13.071825\n"
     ]
    }
   ],
   "source": [
    "# to determine window size, we see the median length of a sentence\n",
    "\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).median())\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda81652",
   "metadata": {},
   "source": [
    "We go with window size of 12 since that is the median length of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f68d8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:47<00:00, 833.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (385005,)\n",
      "contexts.shape: (385005, 11)\n",
      "labels.shape: (385005, 11)\n"
     ]
    }
   ],
   "source": [
    "# note higher window size is more computationally expensive\n",
    "# from documentation, it is said that for small datasets, negative samples of range 5to 20 yields the best results\n",
    "\n",
    "num_ns = 10\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978a0",
   "metadata": {},
   "source": [
    "Configuring training sets for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d037e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 11), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 11), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# randomising the dataset\n",
    "\n",
    "BATCH_SIZE = batch_size\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8fcc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 11), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 11), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9722c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "              embedding_dim,\n",
    "              input_length=1,\n",
    "              name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "               embedding_dim,\n",
    "               input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24ca642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d598c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding size for word2vec is chosen to be of shape (32,)\n",
    "# the idea is that since there are a maximum of 33 words in a sentece\n",
    "# the vector that a word takes will be in 32 dimension \n",
    "# vocab size +1 because of padding\n",
    "embedding_dim = 32\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "# call back to log training stats for TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7df77dbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "375/375 [==============================] - 11s 27ms/step - loss: 2.3874 - accuracy: 0.1242\n",
      "Epoch 2/40\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 2.2944 - accuracy: 0.2151\n",
      "Epoch 3/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 2.1801 - accuracy: 0.2810\n",
      "Epoch 4/40\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 2.0647 - accuracy: 0.3406\n",
      "Epoch 5/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.9434 - accuracy: 0.3902\n",
      "Epoch 6/40\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 1.8218 - accuracy: 0.4332\n",
      "Epoch 7/40\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 1.7063 - accuracy: 0.4724\n",
      "Epoch 8/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.6004 - accuracy: 0.5082\n",
      "Epoch 9/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.5046 - accuracy: 0.5404\n",
      "Epoch 10/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.4180 - accuracy: 0.5698\n",
      "Epoch 11/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.3395 - accuracy: 0.5957\n",
      "Epoch 12/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.2683 - accuracy: 0.6183\n",
      "Epoch 13/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.2035 - accuracy: 0.6388\n",
      "Epoch 14/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.1446 - accuracy: 0.6563\n",
      "Epoch 15/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.0910 - accuracy: 0.6725\n",
      "Epoch 16/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 1.0422 - accuracy: 0.6861\n",
      "Epoch 17/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.9979 - accuracy: 0.6982\n",
      "Epoch 18/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.9575 - accuracy: 0.7091\n",
      "Epoch 19/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.9208 - accuracy: 0.7193\n",
      "Epoch 20/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.8874 - accuracy: 0.7287\n",
      "Epoch 21/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.8570 - accuracy: 0.7374\n",
      "Epoch 22/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.8292 - accuracy: 0.7452\n",
      "Epoch 23/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.8038 - accuracy: 0.7530\n",
      "Epoch 24/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.7804 - accuracy: 0.7601\n",
      "Epoch 25/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.7590 - accuracy: 0.7666\n",
      "Epoch 26/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.7392 - accuracy: 0.7726\n",
      "Epoch 27/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.7209 - accuracy: 0.7782\n",
      "Epoch 28/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.7040 - accuracy: 0.7837\n",
      "Epoch 29/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6882 - accuracy: 0.7887\n",
      "Epoch 30/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6736 - accuracy: 0.7929\n",
      "Epoch 31/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6599 - accuracy: 0.7969\n",
      "Epoch 32/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6471 - accuracy: 0.8007\n",
      "Epoch 33/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6351 - accuracy: 0.8043\n",
      "Epoch 34/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6238 - accuracy: 0.8077\n",
      "Epoch 35/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6132 - accuracy: 0.8108\n",
      "Epoch 36/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.6032 - accuracy: 0.8137\n",
      "Epoch 37/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.5938 - accuracy: 0.8163\n",
      "Epoch 38/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.5848 - accuracy: 0.8186\n",
      "Epoch 39/40\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.5764 - accuracy: 0.8210\n",
      "Epoch 40/40\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.5684 - accuracy: 0.8233\n"
     ]
    }
   ],
   "source": [
    "history = word2vec.fit(dataset, epochs=40, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1ea3",
   "metadata": {},
   "source": [
    "## Writing vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fac836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5568f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('./data/vectors1.tsv', 'w')\n",
    "out_m = io.open('./data/metadata1.tsv', 'w')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7c0b",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52cb7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        result.append([float(value) for value in line.split()])\n",
    "        \n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc7b6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = read_tsv('./data/vectors1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4182c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "            \n",
    "        result.append(line.strip())\n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ee5548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = read_meta('./data/metadata1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09a157dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53611"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150b508",
   "metadata": {},
   "source": [
    "## Preprocessing for GRU: one hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f59f854e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels:\n",
      "0) empty\n",
      "1) sadness\n",
      "2) enthusiasm\n",
      "3) neutral\n",
      "4) worry\n",
      "5) surprise\n",
      "6) love\n",
      "7) fun\n",
      "8) hate\n",
      "9) happiness\n",
      "10) boredom\n",
      "11) relief\n",
      "12) anger\n"
     ]
    }
   ],
   "source": [
    "print('Unique labels:')\n",
    "for i, label in enumerate(cleaned_df['sentiment'].unique()):\n",
    "    print('{}) {}'.format(i,label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f0ffb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e0625",
   "metadata": {},
   "source": [
    "# Build a one hot vector each of size 13 for sentiments as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7034d0",
   "metadata": {},
   "source": [
    "### build a map for the labels to the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24674dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels -> indices\n",
    "sentiment_labels = cleaned_df['sentiment'].unique().copy()\n",
    "\n",
    "# index key -> class\n",
    "label_map = {}\n",
    "\n",
    "#class -> index key\n",
    "inverse_label_map = {}\n",
    "for i, label in enumerate(sentiment_labels):\n",
    "    label_map[i] = label \n",
    "    inverse_label_map[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "080fa0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all keys\n",
    "indices = []\n",
    "for key,value in label_map.items():\n",
    "    indices.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c2016",
   "metadata": {},
   "source": [
    "### Generates one-hot vector for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5467ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the word to the one hot vector\n",
    "depth = len(indices)\n",
    "one_hot_encoding = tf.one_hot(indices,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d31c9844",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 13), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1d9fe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 13])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06199e6",
   "metadata": {},
   "source": [
    "Map the one hot encodings to the labels in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "263b5a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'sadness', ..., 'love', 'happiness', 'love'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79f4929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:05<00:00, 7124.57it/s]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    labels.append(o_h)\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54762954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 13)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d96952",
   "metadata": {},
   "source": [
    "# instantiate the preprocessed dataframe to feed into the model\n",
    "# cleaned_df['label'] = cleaned_df['sentiment'].apply(lambda n: one_hot_encoding[inverse_label_map[n]])\n",
    "# cleaned_df['label'] = cleaned_df['sentiment'].apply(lambda n: inverse_label_map[n])\n",
    "\n",
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    inner_list = []\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    for i in range(0,33):\n",
    "        inner_list.append(o_h)\n",
    "    labels.append(inner_list)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "#labels = np.asarray([(one_hot_encoding[inverse_label_map[sentiment]]) for sentiment in cleaned_df['sentiment'].values])\n",
    "# labels = np.asarray([tf.keras.utils.to_categorical([inverse_label_map[sentiment]],33) for sentiment in cleaned_df['sentiment'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518cb90",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f7269",
   "metadata": {},
   "source": [
    "## Convert Content to tokenized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89e4727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53610 unique tokens\n",
      "labels: [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "content_lines = list(cleaned_df['content'])\n",
    "tokeniser_obj = Tokenizer()\n",
    "tokeniser_obj.fit_on_texts(content_lines)\n",
    "sequences = tokeniser_obj.texts_to_sequences(content_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokeniser_obj.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "\n",
    "max_length = max_sentence_len\n",
    "content_pad = pad_sequences(sequences,maxlen=max_length)\n",
    "print(\"labels: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b31f7773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of content tensor: (40000, 33)\n",
      "shape of sentiment tensor: (40000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of content tensor: {}\".format(content_pad.shape))\n",
    "print(\"shape of sentiment tensor: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a865e",
   "metadata": {},
   "source": [
    "## Map embeddings from word2vec model for each word to the sequences (sentences) by createing a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70488474",
   "metadata": {},
   "source": [
    "Map the embeddings to each word of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88f878f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an inverse index of the classes name -> index\n",
    "word_vector_map = {}\n",
    "for index,word in enumerate(classes):\n",
    "    word_vector_map[word] = vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "442fd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=32\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6f0f15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53611, 32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de7751be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words successfully mapped to vectors: 53610/53612\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word,i in word_index.items():\n",
    "    if i> num_words:\n",
    "        continue\n",
    "    embedding_vector = word_vector_map.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        # if embedding vector is not found, \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count+=1\n",
    "print(\"number of words successfully mapped to vectors: {}/{}\".format(count,vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156385b",
   "metadata": {},
   "source": [
    "# Model of GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbbe53",
   "metadata": {},
   "source": [
    "x (no_sequences,embeddings in a sequence=32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbc58f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to save the best weights for the model\n",
    "def saveWeightsCallback(path,monitor,mode,save_freq):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path,\n",
    "        monitor = monitor,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = mode,\n",
    "        save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f33f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def build_model(vocab_size,embedding_dim,max_length):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(num_words,\n",
    "                               EMBEDDING_DIM,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length = max_length,\n",
    "                              trainable=False))\n",
    "    model.add(layers.GRU(128,return_sequences=True))\n",
    "    model.add(layers.GRU(128,return_sequences=True))\n",
    "    model.add(layers.GRU(128)) \n",
    "    model.add(layers.Dense(13,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e54e8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 33, 32)            1715552   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 33, 128)           62208     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 33, 128)           99072     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977,581\n",
      "Trainable params: 262,029\n",
      "Non-trainable params: 1,715,552\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 33, 32)            1715552   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 33, 128)           62208     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 33, 128)           99072     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977,581\n",
      "Trainable params: 262,029\n",
      "Non-trainable params: 1,715,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GRU_model = build_model(vocab_size,EMBEDDING_DIM,max_length)\n",
    "\n",
    "GRU_model.compile(\n",
    "    optimizer= Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "    ]\n",
    ")\n",
    "\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3dec5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "indices = np.arange(content_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "content_pad = content_pad[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * content_pad.shape[0])\n",
    "\n",
    "X_train_pad = content_pad[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "X_test_pad = content_pad[-num_validation_samples:]\n",
    "y_test = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3fdd92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 - 27s - loss: 2.2174 - accuracy: 0.2358 - val_loss: 2.1405 - val_accuracy: 0.2632 - 27s/epoch - 122ms/step\n",
      "Epoch 2/100\n",
      "219/219 - 22s - loss: 2.1104 - accuracy: 0.2614 - val_loss: 2.0859 - val_accuracy: 0.2688 - 22s/epoch - 100ms/step\n",
      "Epoch 3/100\n",
      "219/219 - 28s - loss: 2.0728 - accuracy: 0.2756 - val_loss: 2.0656 - val_accuracy: 0.2841 - 28s/epoch - 129ms/step\n",
      "Epoch 4/100\n",
      "219/219 - 33s - loss: 2.0602 - accuracy: 0.2838 - val_loss: 2.0602 - val_accuracy: 0.2906 - 33s/epoch - 150ms/step\n",
      "Epoch 5/100\n",
      "219/219 - 36s - loss: 2.0522 - accuracy: 0.2846 - val_loss: 2.0523 - val_accuracy: 0.2908 - 36s/epoch - 166ms/step\n",
      "Epoch 6/100\n",
      "219/219 - 36s - loss: 2.0464 - accuracy: 0.2860 - val_loss: 2.0464 - val_accuracy: 0.2930 - 36s/epoch - 164ms/step\n",
      "Epoch 7/100\n",
      "219/219 - 47s - loss: 2.0422 - accuracy: 0.2895 - val_loss: 2.0451 - val_accuracy: 0.2892 - 47s/epoch - 215ms/step\n",
      "Epoch 8/100\n",
      "219/219 - 49s - loss: 2.0379 - accuracy: 0.2899 - val_loss: 2.0407 - val_accuracy: 0.2957 - 49s/epoch - 225ms/step\n",
      "Epoch 9/100\n",
      "219/219 - 52s - loss: 2.0348 - accuracy: 0.2920 - val_loss: 2.0382 - val_accuracy: 0.2962 - 52s/epoch - 238ms/step\n",
      "Epoch 10/100\n",
      "219/219 - 51s - loss: 2.0296 - accuracy: 0.2928 - val_loss: 2.0359 - val_accuracy: 0.2952 - 51s/epoch - 232ms/step\n",
      "Epoch 11/100\n",
      "219/219 - 51s - loss: 2.0274 - accuracy: 0.2956 - val_loss: 2.0338 - val_accuracy: 0.2984 - 51s/epoch - 232ms/step\n",
      "Epoch 12/100\n",
      "219/219 - 52s - loss: 2.0232 - accuracy: 0.2961 - val_loss: 2.0338 - val_accuracy: 0.2978 - 52s/epoch - 237ms/step\n",
      "Epoch 13/100\n",
      "219/219 - 51s - loss: 2.0211 - accuracy: 0.2948 - val_loss: 2.0324 - val_accuracy: 0.2990 - 51s/epoch - 234ms/step\n",
      "Epoch 14/100\n",
      "219/219 - 51s - loss: 2.0182 - accuracy: 0.2976 - val_loss: 2.0291 - val_accuracy: 0.2977 - 51s/epoch - 232ms/step\n",
      "Epoch 15/100\n",
      "219/219 - 52s - loss: 2.0155 - accuracy: 0.2990 - val_loss: 2.0270 - val_accuracy: 0.3001 - 52s/epoch - 237ms/step\n",
      "Epoch 16/100\n",
      "219/219 - 51s - loss: 2.0133 - accuracy: 0.2982 - val_loss: 2.0269 - val_accuracy: 0.2987 - 51s/epoch - 233ms/step\n",
      "Epoch 17/100\n",
      "219/219 - 51s - loss: 2.0099 - accuracy: 0.3021 - val_loss: 2.0265 - val_accuracy: 0.2975 - 51s/epoch - 232ms/step\n",
      "Epoch 18/100\n",
      "219/219 - 52s - loss: 2.0081 - accuracy: 0.3016 - val_loss: 2.0286 - val_accuracy: 0.3006 - 52s/epoch - 237ms/step\n",
      "Epoch 19/100\n",
      "219/219 - 51s - loss: 2.0051 - accuracy: 0.3021 - val_loss: 2.0239 - val_accuracy: 0.2981 - 51s/epoch - 232ms/step\n",
      "Epoch 20/100\n",
      "219/219 - 51s - loss: 2.0030 - accuracy: 0.3034 - val_loss: 2.0211 - val_accuracy: 0.3015 - 51s/epoch - 235ms/step\n",
      "Epoch 21/100\n",
      "219/219 - 52s - loss: 1.9994 - accuracy: 0.3053 - val_loss: 2.0223 - val_accuracy: 0.2987 - 52s/epoch - 237ms/step\n",
      "Epoch 22/100\n",
      "219/219 - 51s - loss: 1.9983 - accuracy: 0.3071 - val_loss: 2.0188 - val_accuracy: 0.3015 - 51s/epoch - 231ms/step\n",
      "Epoch 23/100\n",
      "219/219 - 51s - loss: 1.9953 - accuracy: 0.3049 - val_loss: 2.0217 - val_accuracy: 0.3020 - 51s/epoch - 233ms/step\n",
      "Epoch 24/100\n",
      "219/219 - 53s - loss: 1.9919 - accuracy: 0.3076 - val_loss: 2.0168 - val_accuracy: 0.3022 - 53s/epoch - 244ms/step\n",
      "Epoch 25/100\n",
      "219/219 - 52s - loss: 1.9900 - accuracy: 0.3072 - val_loss: 2.0183 - val_accuracy: 0.3007 - 52s/epoch - 239ms/step\n",
      "Epoch 26/100\n",
      "219/219 - 51s - loss: 1.9866 - accuracy: 0.3081 - val_loss: 2.0249 - val_accuracy: 0.2980 - 51s/epoch - 234ms/step\n",
      "Epoch 27/100\n",
      "219/219 - 52s - loss: 1.9842 - accuracy: 0.3090 - val_loss: 2.0142 - val_accuracy: 0.3024 - 52s/epoch - 236ms/step\n",
      "Epoch 28/100\n",
      "219/219 - 51s - loss: 1.9827 - accuracy: 0.3113 - val_loss: 2.0128 - val_accuracy: 0.3032 - 51s/epoch - 233ms/step\n",
      "Epoch 29/100\n",
      "219/219 - 51s - loss: 1.9791 - accuracy: 0.3112 - val_loss: 2.0152 - val_accuracy: 0.3059 - 51s/epoch - 234ms/step\n",
      "Epoch 30/100\n",
      "219/219 - 52s - loss: 1.9756 - accuracy: 0.3137 - val_loss: 2.0126 - val_accuracy: 0.3041 - 52s/epoch - 237ms/step\n",
      "Epoch 31/100\n",
      "219/219 - 51s - loss: 1.9738 - accuracy: 0.3126 - val_loss: 2.0134 - val_accuracy: 0.3022 - 51s/epoch - 232ms/step\n",
      "Epoch 32/100\n",
      "219/219 - 52s - loss: 1.9708 - accuracy: 0.3129 - val_loss: 2.0152 - val_accuracy: 0.3031 - 52s/epoch - 235ms/step\n",
      "Epoch 33/100\n",
      "219/219 - 51s - loss: 1.9678 - accuracy: 0.3128 - val_loss: 2.0132 - val_accuracy: 0.3042 - 51s/epoch - 235ms/step\n",
      "Epoch 34/100\n",
      "219/219 - 51s - loss: 1.9659 - accuracy: 0.3153 - val_loss: 2.0098 - val_accuracy: 0.3031 - 51s/epoch - 232ms/step\n",
      "Epoch 35/100\n",
      "219/219 - 52s - loss: 1.9625 - accuracy: 0.3158 - val_loss: 2.0099 - val_accuracy: 0.3065 - 52s/epoch - 236ms/step\n",
      "Epoch 36/100\n",
      "219/219 - 51s - loss: 1.9593 - accuracy: 0.3184 - val_loss: 2.0061 - val_accuracy: 0.3073 - 51s/epoch - 234ms/step\n",
      "Epoch 37/100\n",
      "219/219 - 51s - loss: 1.9569 - accuracy: 0.3166 - val_loss: 2.0098 - val_accuracy: 0.3087 - 51s/epoch - 232ms/step\n",
      "Epoch 38/100\n",
      "219/219 - 52s - loss: 1.9548 - accuracy: 0.3186 - val_loss: 2.0051 - val_accuracy: 0.3078 - 52s/epoch - 237ms/step\n",
      "Epoch 39/100\n",
      "219/219 - 51s - loss: 1.9514 - accuracy: 0.3199 - val_loss: 2.0074 - val_accuracy: 0.3090 - 51s/epoch - 234ms/step\n",
      "Epoch 40/100\n",
      "219/219 - 51s - loss: 1.9492 - accuracy: 0.3206 - val_loss: 2.0027 - val_accuracy: 0.3109 - 51s/epoch - 232ms/step\n",
      "Epoch 41/100\n",
      "219/219 - 52s - loss: 1.9462 - accuracy: 0.3217 - val_loss: 2.0044 - val_accuracy: 0.3128 - 52s/epoch - 236ms/step\n",
      "Epoch 42/100\n",
      "219/219 - 51s - loss: 1.9440 - accuracy: 0.3213 - val_loss: 2.0073 - val_accuracy: 0.3093 - 51s/epoch - 234ms/step\n",
      "Epoch 43/100\n",
      "219/219 - 51s - loss: 1.9414 - accuracy: 0.3246 - val_loss: 2.0024 - val_accuracy: 0.3103 - 51s/epoch - 232ms/step\n",
      "Epoch 44/100\n",
      "219/219 - 52s - loss: 1.9402 - accuracy: 0.3263 - val_loss: 2.0072 - val_accuracy: 0.3098 - 52s/epoch - 240ms/step\n",
      "Epoch 45/100\n",
      "219/219 - 51s - loss: 1.9363 - accuracy: 0.3296 - val_loss: 2.0023 - val_accuracy: 0.3112 - 51s/epoch - 233ms/step\n",
      "Epoch 46/100\n",
      "219/219 - 51s - loss: 1.9340 - accuracy: 0.3280 - val_loss: 2.0039 - val_accuracy: 0.3106 - 51s/epoch - 234ms/step\n",
      "Epoch 47/100\n",
      "219/219 - 52s - loss: 1.9310 - accuracy: 0.3294 - val_loss: 2.0003 - val_accuracy: 0.3139 - 52s/epoch - 239ms/step\n",
      "Epoch 48/100\n",
      "219/219 - 51s - loss: 1.9293 - accuracy: 0.3292 - val_loss: 2.0021 - val_accuracy: 0.3096 - 51s/epoch - 232ms/step\n",
      "Epoch 49/100\n",
      "219/219 - 51s - loss: 1.9262 - accuracy: 0.3318 - val_loss: 1.9998 - val_accuracy: 0.3125 - 51s/epoch - 233ms/step\n",
      "Epoch 50/100\n",
      "219/219 - 52s - loss: 1.9230 - accuracy: 0.3338 - val_loss: 2.0030 - val_accuracy: 0.3136 - 52s/epoch - 237ms/step\n",
      "Epoch 51/100\n",
      "219/219 - 51s - loss: 1.9218 - accuracy: 0.3339 - val_loss: 1.9993 - val_accuracy: 0.3155 - 51s/epoch - 233ms/step\n",
      "Epoch 52/100\n",
      "219/219 - 51s - loss: 1.9197 - accuracy: 0.3343 - val_loss: 2.0034 - val_accuracy: 0.3111 - 51s/epoch - 233ms/step\n",
      "Epoch 53/100\n",
      "219/219 - 52s - loss: 1.9161 - accuracy: 0.3371 - val_loss: 2.0041 - val_accuracy: 0.3103 - 52s/epoch - 237ms/step\n",
      "Epoch 54/100\n",
      "219/219 - 51s - loss: 1.9145 - accuracy: 0.3362 - val_loss: 2.0048 - val_accuracy: 0.3123 - 51s/epoch - 233ms/step\n",
      "Epoch 55/100\n",
      "219/219 - 51s - loss: 1.9114 - accuracy: 0.3374 - val_loss: 1.9991 - val_accuracy: 0.3143 - 51s/epoch - 233ms/step\n",
      "Epoch 56/100\n",
      "219/219 - 52s - loss: 1.9093 - accuracy: 0.3375 - val_loss: 1.9997 - val_accuracy: 0.3138 - 52s/epoch - 238ms/step\n",
      "Epoch 57/100\n",
      "219/219 - 51s - loss: 1.9090 - accuracy: 0.3380 - val_loss: 2.0047 - val_accuracy: 0.3135 - 51s/epoch - 232ms/step\n",
      "Epoch 58/100\n",
      "219/219 - 51s - loss: 1.9061 - accuracy: 0.3395 - val_loss: 2.0062 - val_accuracy: 0.3134 - 51s/epoch - 232ms/step\n",
      "Epoch 59/100\n",
      "219/219 - 52s - loss: 1.9034 - accuracy: 0.3418 - val_loss: 2.0046 - val_accuracy: 0.3147 - 52s/epoch - 238ms/step\n",
      "Epoch 60/100\n",
      "219/219 - 51s - loss: 1.8997 - accuracy: 0.3431 - val_loss: 2.0024 - val_accuracy: 0.3113 - 51s/epoch - 232ms/step\n",
      "Epoch 61/100\n",
      "219/219 - 51s - loss: 1.8974 - accuracy: 0.3420 - val_loss: 2.0027 - val_accuracy: 0.3133 - 51s/epoch - 233ms/step\n",
      "Epoch 62/100\n",
      "219/219 - 52s - loss: 1.8955 - accuracy: 0.3430 - val_loss: 2.0046 - val_accuracy: 0.3128 - 52s/epoch - 238ms/step\n",
      "Epoch 63/100\n",
      "219/219 - 51s - loss: 1.8920 - accuracy: 0.3474 - val_loss: 2.0058 - val_accuracy: 0.3088 - 51s/epoch - 233ms/step\n",
      "Epoch 64/100\n",
      "219/219 - 51s - loss: 1.8924 - accuracy: 0.3463 - val_loss: 2.0070 - val_accuracy: 0.3110 - 51s/epoch - 232ms/step\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 - 52s - loss: 1.8877 - accuracy: 0.3486 - val_loss: 2.0071 - val_accuracy: 0.3142 - 52s/epoch - 238ms/step\n",
      "Epoch 66/100\n",
      "219/219 - 51s - loss: 1.8861 - accuracy: 0.3475 - val_loss: 2.0131 - val_accuracy: 0.3133 - 51s/epoch - 232ms/step\n",
      "Epoch 67/100\n",
      "219/219 - 52s - loss: 1.8844 - accuracy: 0.3470 - val_loss: 2.0065 - val_accuracy: 0.3098 - 52s/epoch - 236ms/step\n",
      "Epoch 68/100\n",
      "219/219 - 52s - loss: 1.8811 - accuracy: 0.3492 - val_loss: 2.0073 - val_accuracy: 0.3162 - 52s/epoch - 238ms/step\n",
      "Epoch 69/100\n",
      "219/219 - 51s - loss: 1.8782 - accuracy: 0.3519 - val_loss: 2.0074 - val_accuracy: 0.3140 - 51s/epoch - 233ms/step\n",
      "Epoch 70/100\n",
      "219/219 - 51s - loss: 1.8748 - accuracy: 0.3529 - val_loss: 2.0080 - val_accuracy: 0.3164 - 51s/epoch - 234ms/step\n",
      "Epoch 71/100\n",
      "219/219 - 52s - loss: 1.8728 - accuracy: 0.3545 - val_loss: 2.0076 - val_accuracy: 0.3137 - 52s/epoch - 237ms/step\n",
      "Epoch 72/100\n",
      "219/219 - 51s - loss: 1.8706 - accuracy: 0.3549 - val_loss: 2.0117 - val_accuracy: 0.3154 - 51s/epoch - 232ms/step\n",
      "Epoch 73/100\n",
      "219/219 - 51s - loss: 1.8666 - accuracy: 0.3569 - val_loss: 2.0102 - val_accuracy: 0.3123 - 51s/epoch - 235ms/step\n",
      "Epoch 74/100\n",
      "219/219 - 52s - loss: 1.8659 - accuracy: 0.3564 - val_loss: 2.0119 - val_accuracy: 0.3116 - 52s/epoch - 238ms/step\n",
      "Epoch 75/100\n",
      "219/219 - 51s - loss: 1.8633 - accuracy: 0.3560 - val_loss: 2.0141 - val_accuracy: 0.3153 - 51s/epoch - 233ms/step\n",
      "Epoch 76/100\n",
      "219/219 - 51s - loss: 1.8604 - accuracy: 0.3581 - val_loss: 2.0117 - val_accuracy: 0.3119 - 51s/epoch - 233ms/step\n",
      "Epoch 77/100\n",
      "219/219 - 52s - loss: 1.8578 - accuracy: 0.3602 - val_loss: 2.0143 - val_accuracy: 0.3184 - 52s/epoch - 238ms/step\n",
      "Epoch 78/100\n",
      "219/219 - 51s - loss: 1.8540 - accuracy: 0.3596 - val_loss: 2.0172 - val_accuracy: 0.3170 - 51s/epoch - 233ms/step\n",
      "Epoch 79/100\n",
      "219/219 - 51s - loss: 1.8543 - accuracy: 0.3606 - val_loss: 2.0186 - val_accuracy: 0.3148 - 51s/epoch - 235ms/step\n",
      "Epoch 80/100\n",
      "219/219 - 52s - loss: 1.8512 - accuracy: 0.3610 - val_loss: 2.0230 - val_accuracy: 0.3111 - 52s/epoch - 237ms/step\n",
      "Epoch 81/100\n",
      "219/219 - 51s - loss: 1.8472 - accuracy: 0.3632 - val_loss: 2.0184 - val_accuracy: 0.3122 - 51s/epoch - 233ms/step\n",
      "Epoch 82/100\n",
      "219/219 - 53s - loss: 1.8440 - accuracy: 0.3653 - val_loss: 2.0250 - val_accuracy: 0.3126 - 53s/epoch - 243ms/step\n",
      "Epoch 83/100\n",
      "219/219 - 52s - loss: 1.8411 - accuracy: 0.3649 - val_loss: 2.0264 - val_accuracy: 0.3103 - 52s/epoch - 236ms/step\n",
      "Epoch 84/100\n",
      "219/219 - 51s - loss: 1.8389 - accuracy: 0.3685 - val_loss: 2.0194 - val_accuracy: 0.3127 - 51s/epoch - 234ms/step\n",
      "Epoch 85/100\n",
      "219/219 - 52s - loss: 1.8355 - accuracy: 0.3680 - val_loss: 2.0347 - val_accuracy: 0.3048 - 52s/epoch - 236ms/step\n",
      "Epoch 86/100\n",
      "219/219 - 52s - loss: 1.8358 - accuracy: 0.3637 - val_loss: 2.0274 - val_accuracy: 0.3156 - 52s/epoch - 237ms/step\n",
      "Epoch 87/100\n",
      "219/219 - 51s - loss: 1.8311 - accuracy: 0.3685 - val_loss: 2.0291 - val_accuracy: 0.3083 - 51s/epoch - 233ms/step\n",
      "Epoch 88/100\n",
      "219/219 - 52s - loss: 1.8280 - accuracy: 0.3696 - val_loss: 2.0286 - val_accuracy: 0.3101 - 52s/epoch - 236ms/step\n",
      "Epoch 89/100\n",
      "219/219 - 52s - loss: 1.8247 - accuracy: 0.3736 - val_loss: 2.0343 - val_accuracy: 0.3138 - 52s/epoch - 236ms/step\n",
      "Epoch 90/100\n",
      "219/219 - 52s - loss: 1.8217 - accuracy: 0.3721 - val_loss: 2.0296 - val_accuracy: 0.3096 - 52s/epoch - 235ms/step\n",
      "Epoch 91/100\n",
      "219/219 - 52s - loss: 1.8177 - accuracy: 0.3773 - val_loss: 2.0452 - val_accuracy: 0.3043 - 52s/epoch - 237ms/step\n",
      "Epoch 92/100\n",
      "219/219 - 51s - loss: 1.8149 - accuracy: 0.3767 - val_loss: 2.0310 - val_accuracy: 0.3108 - 51s/epoch - 234ms/step\n",
      "Epoch 93/100\n",
      "219/219 - 51s - loss: 1.8121 - accuracy: 0.3766 - val_loss: 2.0393 - val_accuracy: 0.3078 - 51s/epoch - 233ms/step\n",
      "Epoch 94/100\n",
      "219/219 - 52s - loss: 1.8100 - accuracy: 0.3771 - val_loss: 2.0378 - val_accuracy: 0.3083 - 52s/epoch - 237ms/step\n",
      "Epoch 95/100\n",
      "219/219 - 51s - loss: 1.8059 - accuracy: 0.3781 - val_loss: 2.0380 - val_accuracy: 0.3065 - 51s/epoch - 234ms/step\n",
      "Epoch 96/100\n",
      "219/219 - 51s - loss: 1.8019 - accuracy: 0.3820 - val_loss: 2.0398 - val_accuracy: 0.3052 - 51s/epoch - 234ms/step\n",
      "Epoch 97/100\n",
      "219/219 - 52s - loss: 1.7995 - accuracy: 0.3821 - val_loss: 2.0515 - val_accuracy: 0.3007 - 52s/epoch - 238ms/step\n",
      "Epoch 98/100\n",
      "219/219 - 51s - loss: 1.7960 - accuracy: 0.3825 - val_loss: 2.0455 - val_accuracy: 0.3037 - 51s/epoch - 234ms/step\n",
      "Epoch 99/100\n",
      "219/219 - 51s - loss: 1.7961 - accuracy: 0.3835 - val_loss: 2.0499 - val_accuracy: 0.3071 - 51s/epoch - 234ms/step\n",
      "Epoch 100/100\n",
      "219/219 - 52s - loss: 1.7903 - accuracy: 0.3841 - val_loss: 2.0485 - val_accuracy: 0.3005 - 52s/epoch - 238ms/step\n"
     ]
    }
   ],
   "source": [
    "history = GRU_model.fit(X_train_pad,\n",
    "                        y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs = 100,\n",
    "                        verbose=2,\n",
    "                        validation_data = (X_test_pad,y_test))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "                     saveWeightsCallback(\n",
    "                         path='./weights/GRU_word2vec',\n",
    "                         monitor = 'val_loss',\n",
    "                         mode = 'min',\n",
    "                         save_freq='epoch',\n",
    "                     )],\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e8978",
   "metadata": {},
   "source": [
    "# implementing GRU without word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09305e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 06:10:33.270883: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6400000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "depth = len(indices)\n",
    "one_hot_encoding = tf.one_hot(indices,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d55d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    labels.append(o_h)\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b36b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.7*len(cleaned_df['content']))\n",
    "X_train = cleaned_df.loc[:train_split-1,'content']\n",
    "y_train = labels[:train_split]\n",
    "\n",
    "X_test = cleaned_df.loc[train_split:,'content']\n",
    "y_test = labels[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser_obj = Tokenizer()\n",
    "total_contents = cleaned_df['content']\n",
    "tokeniser_obj.fit_on_texts(total_contents)\n",
    "\n",
    "#pad sequences\n",
    "max_length = max([len(s.split()) for s in total_contents])\n",
    "\n",
    "# define vocan size \n",
    "vocab_size = len(tokeniser_obj.word_index)+1\n",
    "\n",
    "X_train_tokens =  tokeniser_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens =tokeniser_obj.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens,maxlen=max_length,padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen=max_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model.add(layers.Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length))\n",
    "model.add(layers.GRU(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(13,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b68a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=128,epochs=28,validation_data=(X_test_pad,y_test),verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetworks",
   "language": "python",
   "name": "neuralnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
