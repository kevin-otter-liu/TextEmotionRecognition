{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9721736",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a7f41",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f64bea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9f6996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdd27823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv(\"./data/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354d4ec",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Getting rid of the punctuations marks from dataset\n",
    "2. converting all content to lowercase\n",
    "3. converting unicode characters to ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09517d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.copy()\n",
    "\n",
    "# lowercasing 'content' column\n",
    "cleaned_df['content'] = cleaned_df['content'].str.lower()\n",
    "\n",
    "# removing all words with #hastag and @name and urls\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub('@[A-Za-z0-9_]+','',sentence))\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub('#[A-Za-z0-9_]+','',sentence))\n",
    "cleaned_df['content'].apply(lambda sentence: re.sub(r'http\\S+','',sentence))\n",
    "\n",
    "# removing all punctuation marks\n",
    "exclude = set(string.punctuation)\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: regex.sub(\"\",sentence))\n",
    "\n",
    "# removing unicode characters\n",
    "cleaned_df['content'] = cleaned_df['content'].apply(lambda sentence: sentence.encode(\"ascii\",\"ignore\").decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85871b38",
   "metadata": {},
   "source": [
    "1. Now we want to find the total number of unique words (vocab_size) to rationalise the size of a embedding vector\n",
    "1. We also want to find the maximum number of words that are in a sentence to justify the size of the input layer of the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a451c62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length of 'content' is 33\n",
      "total number of words of content is 522873\n",
      "total number of unique words/vocab_size of content is 53611\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = cleaned_df['content'].apply(lambda s: len(s.split())).max()\n",
    "no_of_words = cleaned_df['content'].apply(lambda content: len(content.split())).sum()\n",
    "\n",
    "\n",
    "# count vocab size\n",
    "cache=set()\n",
    "\n",
    "#counting padding and [unkownd] token\n",
    "vocab_size= 1\n",
    "for key,sentence in cleaned_df['content'].items():\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if(word not in cache):\n",
    "            vocab_size+=1\n",
    "            cache.add(word)\n",
    "            \n",
    "print(\"max sentence length of 'content' is {}\".format(max_sentence_len))\n",
    "print(\"total number of words of content is {}\".format(no_of_words))\n",
    "print(\"total number of unique words/vocab_size of content is {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b625945",
   "metadata": {},
   "source": [
    "Since our max sentence length is 33 we will make an embedding of shape (64,) and pad the difference of the sentence. <br>\n",
    "Since it is computationally heavy to have a vector of 63612 in size. <br>\n",
    "We will use a vector of only 64, meaning that in the skip_grams algorithm, the words will only take 64 other context words into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bad8ba",
   "metadata": {},
   "source": [
    "# using word2Vec to get continous vectors to use as embeddings instead of one-hot vectors\n",
    "> using continuous vectors instead of one-hot vectors is better as continuos vectors contain contextual meaning learned from the unsupervised learning performed in the word2vec training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12a678",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfcb67",
   "metadata": {},
   "source": [
    "Combine the steps to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f33c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size,num_ns,vocab_size,seed):\n",
    "    \n",
    "    # each training sentence is appended to these list\n",
    "    targets,contexts,labels = [],[],[]\n",
    "    \n",
    "    # sampling table for vocab_size tokens\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    \n",
    "    # iterate over all sentences in dataset\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "       \n",
    "\n",
    "        # generating positive skip-gram pairs for a sequence\n",
    "        positive_skip_grams,_ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size = vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size, # TODO: change window size\n",
    "            negative_samples=0\n",
    "        )\n",
    "        \n",
    "        # produce negative samples and create training samples (x_train,labels)\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    \n",
    "            # expand context word to frmo dim shape (1,0) to (1,1)\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word],dtype='int64'),1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class, # to tell the which sample is positive\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size, #TODO: may need to change to just the negative samples of the sentence itself instead of the entire vocab\n",
    "                seed=seed,\n",
    "                name='negative_sampling')\n",
    "    \n",
    "    \n",
    "            # building the context and label vectors for a target word\n",
    "            context = tf.concat([tf.squeeze(context_class,1),negative_sampling_candidates],0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "    \n",
    "    \n",
    "            # append each element from the training ex to global lists\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    \n",
    "    \n",
    "    return targets, contexts, labels \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb6ce",
   "metadata": {},
   "source": [
    "## Preparing training data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fed3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of one sentence is 33 but we just use 64\n",
    "sequence_length = 33\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f630be1",
   "metadata": {},
   "source": [
    "## Tokenise the words in content according to their indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d7fbd",
   "metadata": {},
   "source": [
    "### replace words with their respective tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bc51038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset of all sentences\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(cleaned_df['content'])\n",
    "vectorize_layer.adapt(text_ds.batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b82457",
   "metadata": {},
   "source": [
    "### build a inverse vocab which maps indexes -> words which can be handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56fdc089",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0e9f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "# prefetch does fetching of data and training at the same time using multiple thread\n",
    "# improving performance\n",
    "text_vector_ds = text_ds.batch(batch_size).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8623f6",
   "metadata": {},
   "source": [
    "As you can see we have successfully vectorised our sentences/sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f064d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208726b",
   "metadata": {},
   "source": [
    "## Using unsupervised learning (word2Vec skip_gram) to predict context from targets. \n",
    ">While doing so, we are also training the weights on the embeddings. We can increase window size so that the embeddings learn more contextutal knowledge with respect to the words around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63b60b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "13.071825\n"
     ]
    }
   ],
   "source": [
    "# to determine window size, we see the median length of a sentence\n",
    "\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).median())\n",
    "print(cleaned_df['content'].apply(lambda s: len(s.split())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda81652",
   "metadata": {},
   "source": [
    "We go with window size of 7 since that the median length of a sentence is 12. we want to offer 5 slotssome space for negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f68d8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [02:20<00:00, 285.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1089369,)\n",
      "contexts.shape: (1089369, 6)\n",
      "labels.shape: (1089369, 6)\n"
     ]
    }
   ],
   "source": [
    "# note higher window size is more computationally expensive\n",
    "# from documentation, it is said that for small datasets, negative samples of range 5to 20 yields the best results\n",
    "\n",
    "WINDOW_SIZE = 7\n",
    "num_ns = 5\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978a0",
   "metadata": {},
   "source": [
    "## Configuring training sets for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d037e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(256,), dtype=tf.int64, name=None), TensorSpec(shape=(256, 6), dtype=tf.int64, name=None)), TensorSpec(shape=(256, 6), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# batching the dataset\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8fcc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(256,), dtype=tf.int64, name=None), TensorSpec(shape=(256, 6), dtype=tf.int64, name=None)), TensorSpec(shape=(256, 6), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9722c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "              embedding_dim,\n",
    "              input_length=1,\n",
    "              name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "               embedding_dim,\n",
    "               input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d598c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding size for word2vec is chosen to be of shape (32,)\n",
    "# the idea is that since there are a maximum of 33 words in a sentece\n",
    "# the vector that a word takes will be in 32 dimension \n",
    "# vocab size +1 because of padding\n",
    "embedding_dim = 50\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "# call back to log training stats for TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7df77dbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4255/4255 [==============================] - 158s 37ms/step - loss: 1.6636 - accuracy: 0.2817\n",
      "Epoch 2/15\n",
      "4255/4255 [==============================] - 157s 37ms/step - loss: 1.5417 - accuracy: 0.3807\n",
      "Epoch 3/15\n",
      "4255/4255 [==============================] - 161s 38ms/step - loss: 1.3712 - accuracy: 0.4876\n",
      "Epoch 4/15\n",
      "4255/4255 [==============================] - 162s 38ms/step - loss: 1.1944 - accuracy: 0.5696\n",
      "Epoch 5/15\n",
      "4255/4255 [==============================] - 158s 37ms/step - loss: 1.0417 - accuracy: 0.6315\n",
      "Epoch 6/15\n",
      "4255/4255 [==============================] - 159s 37ms/step - loss: 0.9181 - accuracy: 0.6777\n",
      "Epoch 7/15\n",
      "4255/4255 [==============================] - 157s 37ms/step - loss: 0.8207 - accuracy: 0.7124\n",
      "Epoch 8/15\n",
      "4255/4255 [==============================] - 157s 37ms/step - loss: 0.7446 - accuracy: 0.7391\n",
      "Epoch 9/15\n",
      "4255/4255 [==============================] - 157s 37ms/step - loss: 0.6849 - accuracy: 0.7594\n",
      "Epoch 10/15\n",
      "4255/4255 [==============================] - 157s 37ms/step - loss: 0.6375 - accuracy: 0.7753\n",
      "Epoch 11/15\n",
      "4255/4255 [==============================] - 187s 44ms/step - loss: 0.5991 - accuracy: 0.7880\n",
      "Epoch 12/15\n",
      "4255/4255 [==============================] - 180s 42ms/step - loss: 0.5676 - accuracy: 0.7983\n",
      "Epoch 13/15\n",
      "4255/4255 [==============================] - 158s 37ms/step - loss: 0.5413 - accuracy: 0.8068\n",
      "Epoch 14/15\n",
      "4255/4255 [==============================] - 158s 37ms/step - loss: 0.5191 - accuracy: 0.8139\n",
      "Epoch 15/15\n",
      "4255/4255 [==============================] - 158s 37ms/step - loss: 0.5001 - accuracy: 0.8200\n"
     ]
    }
   ],
   "source": [
    "history = word2vec.fit(dataset, epochs=15, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "188152c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = history.history['loss']\n",
    "x = [i+1 for i in range(0,len(y))]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1ea3",
   "metadata": {},
   "source": [
    "## Writing vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fac836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5568f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('./data/vectors1.tsv', 'w')\n",
    "out_m = io.open('./data/metadata1.tsv', 'w')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7c0b",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "52cb7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        result.append([float(value) for value in line.split()])\n",
    "        \n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc7b6cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 21:17:13.539047: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-03 21:17:13.542948: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-03 21:17:13.553089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-03 21:17:13.562783: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "vectors = read_tsv('./data/vectors1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4182c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta(path):\n",
    "    result = []\n",
    "    file = open(path,'r')\n",
    "    while (True):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "            \n",
    "        result.append(line.strip())\n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "76ee5548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = read_meta('./data/metadata1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09a157dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53610"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150b508",
   "metadata": {},
   "source": [
    "## Preprocessing for GRU: one hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f59f854e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels:\n",
      "0) empty\n",
      "1) sadness\n",
      "2) enthusiasm\n",
      "3) neutral\n",
      "4) worry\n",
      "5) surprise\n",
      "6) love\n",
      "7) fun\n",
      "8) hate\n",
      "9) happiness\n",
      "10) boredom\n",
      "11) relief\n",
      "12) anger\n"
     ]
    }
   ],
   "source": [
    "print('Unique labels:')\n",
    "for i, label in enumerate(cleaned_df['sentiment'].unique()):\n",
    "    print('{}) {}'.format(i,label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f0ffb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e0625",
   "metadata": {},
   "source": [
    "# Build a one hot vector each of size 13 for sentiments as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7034d0",
   "metadata": {},
   "source": [
    "### build a map for the labels to the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24674dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels -> indices\n",
    "sentiment_labels = cleaned_df['sentiment'].unique().copy()\n",
    "\n",
    "# index key -> class\n",
    "label_map = {}\n",
    "\n",
    "#class -> index key\n",
    "inverse_label_map = {}\n",
    "for i, label in enumerate(sentiment_labels):\n",
    "    label_map[i] = label \n",
    "    inverse_label_map[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "080fa0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all keys\n",
    "indices = []\n",
    "for key,value in label_map.items():\n",
    "    indices.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c2016",
   "metadata": {},
   "source": [
    "### Generates one-hot vector for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f5467ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the word to the one hot vector\n",
    "depth = len(indices)\n",
    "one_hot_encoding = tf.one_hot(indices,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d31c9844",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 13), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1d9fe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 13])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06199e6",
   "metadata": {},
   "source": [
    "Map the one hot encodings to the labels in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "263b5a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'sadness', ..., 'love', 'happiness', 'love'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "79f4929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:04<00:00, 8911.85it/s]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    labels.append(o_h)\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "54762954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 13)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518cb90",
   "metadata": {},
   "source": [
    "Getting embeddings using Word2Vec. Word2vec has 2 algorithms, \n",
    "1. Continuous Bag of words\n",
    "    - word is predicted from context \"__ my name is Kevin\"\n",
    "2. Skip Gram\n",
    "    - context is predicted from target \"Hi __ __ __ __\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f7269",
   "metadata": {},
   "source": [
    "## Convert Content to tokenized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "89e4727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53610 unique tokens\n",
      "labels: [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "content_lines = list(cleaned_df['content'])\n",
    "tokeniser_obj = Tokenizer()\n",
    "tokeniser_obj.fit_on_texts(content_lines)\n",
    "sequences = tokeniser_obj.texts_to_sequences(content_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokeniser_obj.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "\n",
    "max_length = max_sentence_len\n",
    "content_pad = pad_sequences(sequences,maxlen=max_length)\n",
    "print(\"labels: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b31f7773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of content tensor: (40000, 33)\n",
      "shape of sentiment tensor: (40000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of content tensor: {}\".format(content_pad.shape))\n",
    "print(\"shape of sentiment tensor: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a865e",
   "metadata": {},
   "source": [
    "## Map embeddings from word2vec model for each word to the sequences (sentences) by creating a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70488474",
   "metadata": {},
   "source": [
    "Map the embeddings to each word of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "88f878f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an inverse index of the classes name -> index\n",
    "word_vector_map = {}\n",
    "for index,word in enumerate(classes):\n",
    "    word_vector_map[word] = vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "442fd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=50\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6f0f15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53611, 50)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de7751be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "number of words successfully mapped to vectors: 53609/53611\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word,i in word_index.items():\n",
    "    if i> num_words:\n",
    "        continue\n",
    "    embedding_vector = word_vector_map.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        # if embedding vector is not found, \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count+=1\n",
    "    else:\n",
    "        print(word)\n",
    "print(\"number of words successfully mapped to vectors: {}/{}\".format(count,vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156385b",
   "metadata": {},
   "source": [
    "# Model of GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbbe53",
   "metadata": {},
   "source": [
    "x (no_sequences,embeddings in a sequence=32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cbc58f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to save the best weights for the model\n",
    "def saveWeightsCallback(path,monitor,mode,save_freq):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path,\n",
    "        monitor = monitor,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = mode,\n",
    "        save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f33f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def build_model(vocab_size,embedding_dim,max_length):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(num_words,\n",
    "                               EMBEDDING_DIM,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length = max_length,\n",
    "                              trainable=False))\n",
    "    model.add(layers.GRU(128,return_sequences=True))\n",
    "    model.add(layers.GRU(128,return_sequences=True))\n",
    "    model.add(layers.GRU(128)) \n",
    "    model.add(layers.Dense(13,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e54e8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 33, 50)            2680550   \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 33, 128)           69120     \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 33, 128)           99072     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,949,491\n",
      "Trainable params: 268,941\n",
      "Non-trainable params: 2,680,550\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 33, 50)            2680550   \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 33, 128)           69120     \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 33, 128)           99072     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,949,491\n",
      "Trainable params: 268,941\n",
      "Non-trainable params: 2,680,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GRU_model = build_model(vocab_size,EMBEDDING_DIM,max_length)\n",
    "\n",
    "GRU_model.compile(\n",
    "    optimizer= Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "    ]\n",
    ")\n",
    "\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3dec5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "indices = np.arange(content_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "content_pad = content_pad[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * content_pad.shape[0])\n",
    "\n",
    "X_train_pad = content_pad[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "X_test_pad = content_pad[-num_validation_samples:]\n",
    "y_test = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3fdd92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "875/875 - 58s - loss: 2.0097 - accuracy: 0.3022 - val_loss: 2.0361 - val_accuracy: 0.3013 - 58s/epoch - 66ms/step\n",
      "Epoch 2/20\n",
      "875/875 - 62s - loss: 1.9980 - accuracy: 0.3060 - val_loss: 2.0223 - val_accuracy: 0.3061 - 62s/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "875/875 - 67s - loss: 1.9911 - accuracy: 0.3101 - val_loss: 2.0178 - val_accuracy: 0.3073 - 67s/epoch - 76ms/step\n",
      "Epoch 4/20\n",
      "875/875 - 59s - loss: 1.9858 - accuracy: 0.3125 - val_loss: 2.0158 - val_accuracy: 0.3087 - 59s/epoch - 67ms/step\n",
      "Epoch 5/20\n",
      "875/875 - 59s - loss: 1.9791 - accuracy: 0.3118 - val_loss: 2.0161 - val_accuracy: 0.3085 - 59s/epoch - 67ms/step\n",
      "Epoch 6/20\n",
      "875/875 - 59s - loss: 1.9746 - accuracy: 0.3171 - val_loss: 2.0098 - val_accuracy: 0.3085 - 59s/epoch - 67ms/step\n",
      "Epoch 7/20\n",
      "875/875 - 59s - loss: 1.9691 - accuracy: 0.3189 - val_loss: 2.0108 - val_accuracy: 0.3090 - 59s/epoch - 67ms/step\n",
      "Epoch 8/20\n",
      "875/875 - 59s - loss: 1.9639 - accuracy: 0.3195 - val_loss: 2.0085 - val_accuracy: 0.3056 - 59s/epoch - 67ms/step\n",
      "Epoch 9/20\n",
      "875/875 - 59s - loss: 1.9600 - accuracy: 0.3209 - val_loss: 2.0052 - val_accuracy: 0.3126 - 59s/epoch - 67ms/step\n",
      "Epoch 10/20\n",
      "875/875 - 59s - loss: 1.9541 - accuracy: 0.3210 - val_loss: 2.0088 - val_accuracy: 0.3086 - 59s/epoch - 67ms/step\n",
      "Epoch 11/20\n",
      "875/875 - 59s - loss: 1.9496 - accuracy: 0.3239 - val_loss: 2.0078 - val_accuracy: 0.3095 - 59s/epoch - 67ms/step\n",
      "Epoch 12/20\n",
      "875/875 - 59s - loss: 1.9431 - accuracy: 0.3267 - val_loss: 2.0007 - val_accuracy: 0.3117 - 59s/epoch - 67ms/step\n",
      "Epoch 13/20\n",
      "875/875 - 59s - loss: 1.9392 - accuracy: 0.3264 - val_loss: 2.0006 - val_accuracy: 0.3106 - 59s/epoch - 67ms/step\n",
      "Epoch 14/20\n",
      "875/875 - 59s - loss: 1.9330 - accuracy: 0.3302 - val_loss: 2.0015 - val_accuracy: 0.3128 - 59s/epoch - 67ms/step\n",
      "Epoch 15/20\n",
      "875/875 - 59s - loss: 1.9266 - accuracy: 0.3301 - val_loss: 1.9989 - val_accuracy: 0.3119 - 59s/epoch - 67ms/step\n",
      "Epoch 16/20\n",
      "875/875 - 59s - loss: 1.9211 - accuracy: 0.3338 - val_loss: 1.9966 - val_accuracy: 0.3142 - 59s/epoch - 67ms/step\n",
      "Epoch 17/20\n",
      "875/875 - 59s - loss: 1.9141 - accuracy: 0.3356 - val_loss: 1.9931 - val_accuracy: 0.3142 - 59s/epoch - 67ms/step\n",
      "Epoch 18/20\n",
      "875/875 - 59s - loss: 1.9077 - accuracy: 0.3393 - val_loss: 1.9956 - val_accuracy: 0.3146 - 59s/epoch - 67ms/step\n",
      "Epoch 19/20\n",
      "875/875 - 59s - loss: 1.9008 - accuracy: 0.3408 - val_loss: 2.0023 - val_accuracy: 0.3100 - 59s/epoch - 67ms/step\n",
      "Epoch 20/20\n",
      "875/875 - 59s - loss: 1.8945 - accuracy: 0.3435 - val_loss: 1.9955 - val_accuracy: 0.3167 - 59s/epoch - 67ms/step\n"
     ]
    }
   ],
   "source": [
    "history = GRU_model.fit(X_train_pad,\n",
    "                        y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs = 20,\n",
    "                        verbose=2,\n",
    "                        validation_data = (X_test_pad,y_test))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "                     saveWeightsCallback(\n",
    "                         path='./weights/GRU_word2vec',\n",
    "                         monitor = 'val_loss',\n",
    "                         mode = 'min',\n",
    "                         save_freq='epoch',\n",
    "                     )],\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1454ba6",
   "metadata": {},
   "source": [
    "# Plot loss and accuracy of GRU Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0a828cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = history.history['val_accuracy']\n",
    "x = [i+1 for i in range(0,len(y))]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e8978",
   "metadata": {},
   "source": [
    "# implementing GRU without word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09305e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 06:10:33.270883: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6400000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "depth = len(indices)\n",
    "one_hot_encoding = tf.one_hot(indices,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d55d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for sentiment in tqdm.tqdm(cleaned_df['sentiment'].values):\n",
    "    o_h = one_hot_encoding[inverse_label_map[sentiment]]\n",
    "    labels.append(o_h)\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b36b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.7*len(cleaned_df['content']))\n",
    "X_train = cleaned_df.loc[:train_split-1,'content']\n",
    "y_train = labels[:train_split]\n",
    "\n",
    "X_test = cleaned_df.loc[train_split:,'content']\n",
    "y_test = labels[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser_obj = Tokenizer()\n",
    "total_contents = cleaned_df['content']\n",
    "tokeniser_obj.fit_on_texts(total_contents)\n",
    "\n",
    "#pad sequences\n",
    "max_length = max([len(s.split()) for s in total_contents])\n",
    "\n",
    "# define vocan size \n",
    "vocab_size = len(tokeniser_obj.word_index)+1\n",
    "\n",
    "X_train_tokens =  tokeniser_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens =tokeniser_obj.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens,maxlen=max_length,padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen=max_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model.add(layers.Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length))\n",
    "model.add(layers.GRU(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(13,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b68a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=128,epochs=28,validation_data=(X_test_pad,y_test),verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetworks",
   "language": "python",
   "name": "neuralnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
